# -*- coding: utf-8 -*-
"""Churn Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MFM7t-9BGTflstVqMnih6bWhU8BHbxLn

#EXPLORATORY DATA ANALYSIS
"""

# Import libraries for EDA
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("/churn.csv")

df.head()

df.shape

df.isna().sum()

df.dtypes

"""Removing columns that do not provide relevant information for predicting churn."""

df.drop(columns=['RowNumber', 'CustomerId', 'Surname'], inplace=True)
df.columns

df.describe()

"""We get the number of unique values for each of the features in the data"""

from prettytable import PrettyTable

table = PrettyTable()
table.field_names = ["Features","Unique Values"]
for i in list(df.columns) :
    nunique =df[str(i)].nunique
    table.add_row([i, f"{nunique()}"])
print('Unique values in the dataset : \n')
print(table)

"""We get the unique values for each categorical and binary features"""

columns_of_interest = ['Geography', 'Gender', 'NumOfProducts', 'HasCrCard',
                       'IsActiveMember', 'Exited']

# Get unique values for each column
unique_values = {}
for column in columns_of_interest:
    unique_values[column] = df[column].unique()

# Print unique values for each column
for column, values in unique_values.items():
    print(f"{column}: {values}")

# Proportion of churned To retained customers
exit_counts = df['Exited'].value_counts(normalize=True)
exit_counts

"""#DATA VISUALIZATION

We visualize the proportion of churned to retained customers
"""

plt.figure(figsize=(8, 6))
plt.pie(exit_counts, labels=exit_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Proportion of Churned and Retained Customers')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

"""We select features with numeric continuous values (integer and float) and discrete values differently so that  appropriate methods/visualization could be applied to each type of feature

We'll start with the numeric continuous values (integer and float)
"""

df_numeric = df.select_dtypes(include=[int, float])
df_numeric.head(3)

len(df_numeric.columns)

"""Visualization of the numeric features from the dataframe"""

df_numeric.hist(bins=40,figsize=(12,8))
plt.tight_layout()
plt.show()

cont_col = df_numeric[['CreditScore', 'Age', 'Balance', 'EstimatedSalary']]

fig, axes = plt.subplots(nrows=len(cont_col.columns) // 2, ncols=2, figsize=(12, 8))

for idx, column in enumerate(cont_col.columns):
    row_idx = idx // 2
    col_idx = idx % 2
    sns.boxplot(x='Exited', y=column, data=df_numeric, ax=axes[row_idx, col_idx])
    axes[row_idx, col_idx].set_xlabel('Exited')
    axes[row_idx, col_idx].set_ylabel(column)
    axes[row_idx, col_idx].set_title(f"{column} Distribution")

plt.tight_layout()
plt.show()

cont_col = df_numeric[['CreditScore', 'Age', 'Balance', 'EstimatedSalary', 'Tenure']]

# Set up the matplotlib figure
plt.figure(figsize=(15, 12))

# Loop through the number of columns and create KDE plots
for i, col in enumerate(cont_col.columns):
    # Create a new subplot for each column
    plt.subplot(3, 2, i + 1)
    sns.histplot(data=df_numeric, x=col, fill=True, kde=True, hue='Exited')
    plt.title(f'KDE of {col}')
    plt.tight_layout()

# Show the plots
plt.show()

"""We'll now select features with discrete values"""

# list of columns with disCrete values
disc_cols = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 'NumOfProducts']

# Set up the matplotlib figure and axes for a 2x3 grid
fig, axs = plt.subplots(2, 3, figsize=(15, 10))

# Flatten the array of axes for easy iteration
axs = axs.flatten()

for i, col in enumerate(disc_cols):
    # Create a count plot for each categorical column with respect to Exited on each subplot
    sns.countplot(data=df, x=col, hue='Exited', ax=axs[i])
    axs[i].set_title(f'Distribution of {col} by Exited')
    axs[i].set_xlabel(col)
    axs[i].set_ylabel('Count')

# Remove any unused subplots
for j in range(len(disc_cols), len(axs)):
    fig.delaxes(axs[j])

plt.tight_layout()
plt.show()

"""We use heatmap to visualize the correlation of the various features in the dataframe"""

plt.figure(figsize=(10, 8))
sns.heatmap(df_numeric.corr(), annot=True,cmap="Spectral")
plt.title('Correlation Matrix (Numeric Columns)')
plt.show()

"""#DATA PREPROCESSING FOR MACHINE LEARNING"""

# Importing necessary libraries for machine learning
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import (accuracy_score, f1_score,average_precision_score, confusion_matrix,
                             average_precision_score, precision_score, recall_score, roc_auc_score,\
                             classification_report)
from sklearn.svm import SVC
from xgboost import XGBClassifier

df.head()

"""We transform the categorical features into numerical representation using OrdinalEncoder"""

# Initialize OrdinalEncoder
ordinal_encoder = OrdinalEncoder()

# Columns to encode
columns_to_encode = ['Geography', 'Gender']

# Encode the specified columns
encoded_values = ordinal_encoder.fit_transform(df[columns_to_encode])

# Replace the original columns with the encoded values
df_en = df.copy()
df_en[columns_to_encode] = encoded_values

df_en

"""Splitting the data into training and test set"""

#Create features and labels
labels = df_en["Exited"].values

df_en.drop('Exited', axis=1, inplace=True)
features = df_en.values

#Split data
train_features, test_features, train_labels, test_labels = train_test_split(features, labels,\
                                                                   test_size=0.2, random_state=26)

print(test_features.shape)
train_features.shape

"""Now that all features are numerical, we standardize them so that all features have the same scale"""

# Initialize StandardScaler
scaler = StandardScaler()

# Fit scaler to training features and transform training features
train_features_scaled = scaler.fit_transform(train_features)

# Transform test features using the same scaler
test_features_scaled = scaler.transform(test_features)

train_features_scaled_df = pd.DataFrame(data=train_features_scaled, columns=df_en.columns)
train_features_scaled_df.head()

"""We use the feature_importance attribute of a RandomForestClassifier to understand the relative importance of different features of our data."""

#Create and train a Random Forest Classfier
model=RandomForestClassifier(random_state=26)
model.fit(train_features_scaled,train_labels)
feature_importance=model.feature_importances_

#Create a DataFrame to associate feature names with their importances
feature_importance_df=pd.DataFrame({'Feature':train_features_scaled_df.columns,'Importance':feature_importance})

#Sort feature by importance
feature_importance_df=feature_importance_df.sort_values(by='Importance',ascending=False)
feature_importance_df

plt.figure(figsize=(8, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Feature Name')
plt.title('Feature Importance')
plt.show()

"""#Synthetic Minority Over-sampling Technique (SMOTE)

In our exploratory data analysis (EDA) session, the pie chart revealed that the proportion of 'churn' instances were lower compared to 'not churn' instances. To tackle this class imbalance, we will employ SMOTE to resample the data.
"""

from imblearn.over_sampling import SMOTE

sm = SMOTE(random_state=26)

train_features_sm, train_labels_sm = sm.fit_resample(train_features_scaled, train_labels)

print(f'''Shape of train_features_scaled before SMOTE: {train_features_scaled.shape}
Shape of train_features_scaled after SMOTE: {train_features_sm.shape}''')

print('\nCount  of  positive and negative classes (%): after SMOTE:')
train_labels_smSeries = pd.Series(train_labels_sm)
train_labels_smSeries.value_counts(normalize=True) * 100

"""#HYPERPARAMETER TUNING AND MODEL SELECTION

We'll use hyperparameter tuning and  cross  validation techniques  to optimize  and  evaluate the performance of our  chosen machine learning models  respectively.
"""

# Define models and their respective hyperparameters to tune
models = {
    'Logistic Regression': (LogisticRegression(), {'C': [0.1, 1, 10]}),
    'Decision Tree': (DecisionTreeClassifier(), {'max_depth': [None, 5, 10]}),
    'Random Forest': (RandomForestClassifier(), {'n_estimators': range(50, 600, 50)}),
    'KNN': (KNeighborsClassifier(), {'n_neighbors': range(3, 21, 2)}),
    'SVM': (SVC(), {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}),
    'XGBoost': (XGBClassifier(), {'learning_rate': [0.1, 0.01], 'max_depth': range(3, 30, 3)})
}

# Perform grid search for each model
final_models = {}
for name, (model, params) in models.items():
    grid_search = GridSearchCV(model, params, cv=5, n_jobs=-1)
    grid_search.fit(train_features_sm, train_labels_sm)

    # Retrieve best parameters, best score, and best estimator
    best_params = grid_search.best_params_
    best_score = grid_search.best_score_
    best_estimator = grid_search.best_estimator_

    # Print results
    print(f"Best parameters for {name}: {best_params}")
    print(f"Best cross-validation score for {name}: {best_score}")

    # Store the best estimator
    final_models[name] = best_estimator

"""We use the best hyperparameters for each respective model gotten above to fit, predict, and compute the accuracy, AUC, and F1-score for each model. Note that from the crossvalidation scores, RandomForestClassifier is the best performing model."""

model_name=[]
accuracy=[]
roc_auc=[]
F1_score=[]
#Define models and their best performing respective hyperparameters
models=[
     LogisticRegression(C=0.1, random_state=26),
     DecisionTreeClassifier(max_depth=3, random_state=26),
     RandomForestClassifier(n_estimators=350, random_state=26),
     KNeighborsClassifier(n_neighbors=10),
     #SVC(C=1, kernel='rbf', random_state=26),
     XGBClassifier(learning_rate=0.1, max_depth=9, random_state=26)

]

for model in models:
    model.fit(train_features_sm, train_labels_sm)
    prediction = model.predict(test_features_scaled)
    y_predict=model.predict_proba(test_features_scaled)[:, 1]
    model_name.append(model.__class__.__name__)
    accuracy.append((accuracy_score( prediction , test_labels ) * 100 ) )
    roc_auc.append((roc_auc_score(test_labels , y_predict)*100) )
    F1_score.append((f1_score(test_labels , prediction)* 100) )

model_name_shortened = ['LogisticRegression', 'DecisionTree','RandomForest', 'KNeighbors', 'XGBClassifier']

models_df = pd.DataFrame({"Model-Name":model_name_shortened, "Accuracy": accuracy ,'AUC':roc_auc ,'F1-Score':F1_score})
#models_df = models_df.drop(columns='Model-Name').columns.astype(float)

#models_df.sort_values("AUC", ascending = False)
models_df

"""The best performing model, considering all factors (crossvalidation score, Accuracy, AUC, and F1-Score), is RandomForestClassifier"""

fig, ax = plt.subplots(figsize=(8, 6))
sns.pointplot(x='Model-Name', y='AUC', data=models_df, ax=ax)
ax.set_xticklabels(models_df['Model-Name'], rotation=90)
ax.set_title('Model Comparison: AUC Score')
fig.tight_layout()
plt.show()

fig, ax = plt.subplots(figsize=(8, 6))
sns.pointplot(x='Model-Name', y='F1-Score', data=models_df, ax=ax)
ax.set_xticklabels(models_df['Model-Name'], rotation=90)
ax.set_title('Model Comparison: F1-Score')
fig.tight_layout()
plt.show()

"""#ASSESSING THE PERFORMANCE OF OUR FINAL MODEL

Our final model is the RandomForestClassifier since it's the best performing model considering all factors (crossvalidation scores, accuracy, AUC, f1-score)

###Classification Report
"""

# Using the index of the RandomForestClassifier from the model_name list
rforest_model = model_name[2]

# We store the predictions made by the RForestClassifier in rforest_predictions contains
rforest_prediction = prediction
print(rforest_model)
# 'test_labels' contains the true labels for the test set, so we use it to get the classification report
classification_rep = classification_report(test_labels, rforest_prediction)
print("RandomForestlassifier Classification Report:")
print(classification_rep)

"""###Confusion Matrix"""

#Plot heatmap of the confusion matrix of the RandomForest Classifier
#sns.set(font_scale=1.2)  # Adjust font size
sns.heatmap(confusion_matrix(test_labels, rforest_prediction), annot=True, cmap='Spectral', fmt='.2f', xticklabels=['Not Churn', 'Churn'], yticklabels=['Not Churn', 'Churn'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('RandomForest Confusion Matrix')
plt.show()

